
import os
from datetime import datetime
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from vectorizador import retriever
import time

# --- CONFIGURACI√ìN ---
model = OllamaLLM(model="llama3.2:latest") 
INPUT_FILE = "C:/python_projects/IA_Dalia/Proyecto_3/preguntas.txt"
OUTPUT_FILE = "C:/python_projects/IA_Dalia/Proyecto_3/Informes_modelo/INFORME_FINAL_COMPLETO_2.md"

# --- TEMPLATE DEL PROMPT (Cient√≠fico/Filos√≥fico) ---
template = """
Eres un investigador experto en filosof√≠a y an√°lisis de datos. Proyecto: "La Generaci√≥n Z y la Crisis de Sentido".

OBJETIVO: Responde la siguiente pregunta de investigaci√≥n sintetizando:
1. TEOR√çA: Conceptos filos√≥ficos (Heidegger, Han, Bauman, etc.) presentes en el contexto.
2. EVIDENCIA: Datos emp√≠ricos (YouTube, encuestas, Reddit) presentes en el contexto.

CONTEXTO RECUPERADO:
{context}

PREGUNTA DE INVESTIGACI√ìN: 
{question}

INSTRUCCIONES DE RESPUESTA:
- Escribe un an√°lisis profundo y estructurado (m√≠nimo 2 p√°rrafos).
- Cita las fuentes te√≥ricas y emp√≠ricas expl√≠citamente.
- Si hay contradicciones entre la teor√≠a y los datos, se√±√°lalas.
- Responde en espa√±ol acad√©mico.

AN√ÅLISIS:
"""

prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

def cargar_preguntas(filepath):
    if not os.path.exists(filepath):
        print(f"‚ùå Error: No se encontr√≥ el archivo '{filepath}'")
        return []
    with open(filepath, "r", encoding="utf-8") as f:
        # Lee las l√≠neas y quita los espacios vac√≠os
        return [line.strip() for line in f if line.strip()]

def procesar_cuestionario():
    print("\n=== INICIANDO AN√ÅLISIS AUTOMATIZADO DE PROYECTO DE IA ===")
    
    # 1. Cargar preguntas
    preguntas = cargar_preguntas(INPUT_FILE)
    if not preguntas:
        return

    total = len(preguntas)
    print(f"üìÇ Se encontraron {total} preguntas en '{INPUT_FILE}'.")
    print(f"üìù El resultado se escribir√° en '{OUTPUT_FILE}'\n")

    # 2. Preparar el archivo de salida (Escribir cabecera)
    timestamp = datetime.now().strftime("%d/%m/%Y %H:%M")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(f"# Informe de Investigaci√≥n: Crisis de Sentido Gen Z\n")
        f.write(f"**Fecha de generaci√≥n:** {timestamp}\n")
        f.write(f"**Modelo:** Deepseek-r1 + RAG\n")
        f.write(f"**Total de preguntas:** {total}\n")
        f.write("---\n\n")

    # 3. Bucle de procesamiento
    for i, question in enumerate(preguntas, 1):
        print(f"‚è≥ Procesando pregunta {i}/{total}: {question[:50]}...")
        start_time = time.time()

        # --- A. RECUPERACI√ìN (RAG) ---
        docs = retriever.invoke(question)
        
        # Formatear contexto enriquecido con fuentes
        context_text = ""
        sources_used = set() # Para listar fuentes al final de la respuesta
        
        for doc in docs:
            source_type = doc.metadata.get("source", "desconocido").upper()
            content = doc.page_content.replace("\n", " ")
            context_text += f"[{source_type}]: {content}\n\n"
            sources_used.add(source_type)

        # --- B. GENERACI√ìN (LLM) ---
        response = chain.invoke({"context": context_text, "question": question})

        # --- C. ESCRITURA EN EL ARCHIVO ---
        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(f"## {i}. {question}\n\n")
            f.write(f"{response}\n\n")
            
            # Secci√≥n de Fuentes (Metadatos)
            f.write("**Fuentes consultadas para este an√°lisis:**\n")
            for src in sources_used:
                f.write(f"- *{src}*\n")
            f.write("\n---\n\n") # Separador
        
        elapsed = time.time() - start_time
        print(f"‚úÖ Terminada en {elapsed:.2f}s.\n")

    print(f"\nüéâ ¬°PROCESO COMPLETADO! Revisa el archivo: {OUTPUT_FILE}")

if __name__ == "__main__":
    procesar_cuestionario()